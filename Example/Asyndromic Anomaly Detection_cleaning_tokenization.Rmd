---
title: "Anomaly Detection in Ustructured Data: Data Cleaning"
author: "Anna Bratcher"
date: "`r Sys.Date()`"
output: html_document
---

## **Background**

The purpose of this analysis is to identify distinctive terms in unstructured data that are potentially indicative of outbreaks comparing a specified period of interest ("post") to a specified baseline period ("pre"). 




```{r setup, include=FALSE}

#Prep
  library(tidyverse)
  library(httr)
  library(jsonlite) 
  library(lubridate)
  library(tidylo)
  library(wesanderson)
  library(ggsci)
  library(ggpubr)
  library(janitor)
  library(MMWRweek)
  library(data.table)
  library(tidytext)
  library(ggthemes)
  library(scales)
  library(quanteda)
  library(quanteda.textmodels)
  library(quanteda.textstats)
  library(quanteda.textplots)
  library(cowplot)
  library(ggrepel)
  library(text2vec)
  library(hunspell)
  library(stringi)
  library(qdapRegex)
```


## **Introduction**

Data require, at minimum, a date field and an open text field that you are monitoring for changes over time. 

This example is written for the Covid Vaccine Tweets available at https://www.kaggle.com/datasets/kaushiksuresh147/covidvaccine-tweets. 

We subsetted the columns so that the data has the following structure: 

| user followers   | user_verified       | date         |text                      |
|------------------|--------------------:|-------------:|-------------------------:|
| integer          | “TRUE” or “FALSE”   | date         | free text	               |

			 	         
```{r data pull, echo=FALSE}


raw_data <- read_csv('data/raw_data.csv', show_col_types = FALSE)

```

```{r process data, echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE}

df <- raw_data %>%
  as.data.table() %>%
  .[, date := as.Date(date, "%m/%d/%Y")] %>%
  .[, followers := fcase(between(user_followers, 0, 249), "0-249",
                         between(user_followers, 250, 999), "250-999",
                         between(user_followers, 1000, 4999), "1,000-4,999", 
                         between(user_followers, 5000, 9999), "5,000-9,999", 
                         between(user_followers, 10000, 999999), "10,000-999,999",
                         user_followers >= 1000000, "1,000,000+",
                         default = "Other")]

```




##**Chief complaint cleaning and tokenization** 

Here text is lightly cleaned:  
  -Removal of punctuation 
  -Capitalization 
  -removal of extra white space



```{r clean tweets, echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE}

df <- df %>%
  mutate(
    text = toupper(text),
    text = str_remove_all(text, pattern = "[[:punct:]]"),
    text = str_squish(text),
    text = rm_nchar_words(text, "1,2")) #this line removes words of 1 or 2 characters. Assumes these are non-informative

df$verified <- ifelse(df$user_verified == TRUE, "Verified", "Not Verified")

# For Electronic Health Record (EHR) data, this code translates International Classification of Disease (ICD)
# codes into English descriptions. 

#dictionary <- read_csv("icd10_crosswalk.csv") %>%
#  mutate(set = factor(set, levels = c("ICD-10 2022", "ICD-10 2019", "ICD-10 2009"))) %>%
#  arrange(code, set) %>%
#  distinct(code, .keep_all = TRUE) %>%
#  select(-set)

#df <-  df %>%
#        mutate (across(chief_complaint, ~str_replace_all(.x, deframe(dictionary))))


fwrite(df, paste0("data/df_processed_data.csv"))

#df <- read_csv("data/df_processed_data.csv")

```



##**tokenization of chief complaints**

Tokenization: Splitting long strings of text into smaller units (tokens).

This code also removes punctuation, symbols, numbers, and separators. 

```{r tokenization, echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE}

cc_tokens <- as.data.table(df) %>%
  .[, text := vapply(lapply(str_split(text, " "), unique), paste, character(1L), collapse = " ")] %>%
  corpus(text_field = "text") %>%
  tokens(
    what = "word",
    remove_punct = TRUE, 
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_separators = TRUE,
    verbose = TRUE
  ) %>%
  tokens_select(pattern = c("THE", "AND", "FOR", "WITH"), selection = "remove") #This line removes any non-informative words


```

**NOTE**: Additional code could be added here to correct spelling among tokens. 

Once identified, tokens are organized into unigrams and bigrams. This code also keeps factors you would like to sort by later (here, number of followers for the user and if the user is verified on Twitter)

**Note**: This step can take a bit of time. This is why the output files are saved; so you don't have to run this code every time.  


```{r unigrams/bigrams, echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE}
cc_unigrams <- cc_tokens %>%
  tokens_ngrams(n = 1) %>%
  dfm(tolower = FALSE) %>%
  textstat_frequency(groups = interaction(followers, date, verified, sep = "_")) %>%
  as.data.table() %>%
  .[, c("followers", "date", "verified") := tstrsplit(group, "_", fixed = TRUE)] %>%
  as.data.frame() %>%
  select(
    unigram = feature, 
    date,
    followers,
    verified,
    n = frequency
  ) 

fwrite(cc_unigrams, paste0("data/cc_unigrams.csv"))
```


```{r}
cc_bigrams <- cc_tokens %>%
  tokens_ngrams(n = 2, concatenator = " ") %>%
  dfm(tolower = FALSE) %>%
  textstat_frequency(groups = interaction(followers, date, verified, sep = "_")) %>%
  as.data.table() %>%
  .[, c("followers", "date", "verified") := tstrsplit(group, "_", fixed = TRUE)] %>%
  .[, .(bigram = feature, date = date, followers = followers, verified = verified, n = frequency)] %>%
# .[, c("word1", "word2") := tstrsplit(bigram, " ", fixed = TRUE)] %>%             
# .[, bigram := fifelse(word1 < word2, paste(word1, word2), paste(word2, word1))] %>%
  .[, .(n = sum(n)), by = c("bigram", "date", "followers", "verified")] %>%
  as.data.frame()  

fwrite(cc_bigrams, paste0("data/cc_bigrams.csv"))

```


